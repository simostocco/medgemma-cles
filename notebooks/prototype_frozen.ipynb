{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:00:25.247183Z","iopub.execute_input":"2026-02-10T10:00:25.247446Z","iopub.status.idle":"2026-02-10T10:00:27.37546Z","shell.execute_reply.started":"2026-02-10T10:00:25.247423Z","shell.execute_reply":"2026-02-10T10:00:27.374683Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Project Notes (Use & Limitations)\n\n### Data sources\n- This notebook retrieves bibliographic abstracts from PubMed via NCBI E-utilities.\n- It retrieves molecule and bioactivity metadata from the ChEMBL public API.\n\n### Model access\n- The LLM used (TxGemma) is hosted on Hugging Face and may be gated.\n- This notebook requires that the user has accepted the model license/terms on Hugging Face and provides their own access token.\n\n### What this notebook produces\n- A structured research-style summary grounded ONLY in the retrieved snippets.\n- A citation-coverage metric that checks whether claims include snippet citations.\n\n### What this notebook does NOT do\n- It does not provide medical advice.\n- It does not claim that any drug is effective for any disease.\n- It does not replace reading full papers (abstracts are incomplete summaries).\n\n### Reproducibility\n- Results can vary depending on which abstracts are returned by PubMed at runtime.\n- Cached requests are stored in `/kaggle/working/data_cache` to reduce repeated downloads.\n","metadata":{}},{"cell_type":"markdown","source":"## Project Overview\n\n**MedGemma ‚Äì Closed-Loop Evidence Synthesis (CLES)** is a research-oriented application that implements a specialized Retrieval-Augmented Generation (RAG) architecture to bridge the gap between high-level therapeutic questions and grounded molecular and clinical evidence.\n\nBy leveraging **Google‚Äôs TxGemma-9B-Chat**, the system generates structured, evidence-backed reports describing drug‚Äìdisease relationships while enforcing strict provenance and citation constraints.\n\n---\n\n## Architecture & Methodology\n\nThe pipeline is built around a **dual-engine retrieval strategy**:\n\n### 1) Molecular Grounding\nReal-time bioactivity data are retrieved from the **ChEMBL API** to establish biological plausibility.  \nThis includes:\n- Target interaction density  \n- Standardized bioactivity measurements (e.g., IC‚ÇÖ‚ÇÄ, K·µ¢)  \n- Assay and reference aggregation  \n\nThese data provide a mechanistic foundation for evaluating drug‚Äìtarget relevance.\n\n### 2) Clinical / Textual Grounding\nRelevant biomedical literature is retrieved from **PubMed** using **NCBI E-Utilities**.  \nAbstracts are selected for relevance and recency, supplying clinical and experimental context that complements molecular evidence.\n\n---\n\n## Inference & Safety Design\n\nTo operate within **Kaggle‚Äôs Tesla T4 GPU constraints**, TxGemma-9B is deployed using **4-bit NF4 quantization**, reducing memory usage while preserving inference quality.\n\nA **citation-constrained generation protocol** is enforced via prompt design and post-generation validation:\n- Every factual claim must map to explicit source identifiers (e.g., `[S1]`, `[S2]`)\n- Claims without supporting snippets are flagged or penalized\n- References outside the retrieved evidence set are detected automatically\n\nThis produces a verifiable **provenance trail**, enabling researchers to audit AI-generated reasoning directly against primary literature.\n\n---\n\n## Intended Use & Limitations\n\nThis system is designed for **research support and evidence exploration only**.  \nIt does **not** provide medical advice, clinical recommendations, or treatment validation.\n\nBy tightly coupling generation with retrieval and citation enforcement, MedGemma-CLES significantly reduces‚Äîbut does not eliminate‚Äîthe risk of hallucination inherent to large language models.\n","metadata":{}},{"cell_type":"code","source":"# =========================\n# 0) Environment Setup\n# =========================\n# We pin versions because Kaggle images often contain conflicting packages\n# (TensorFlow, old hub versions, etc.) that can break Transformers imports.\n\nimport os\n\n# Prevent Transformers from trying to import TensorFlow / Flax,\n# which can cause import errors in Kaggle environments.\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n\n# Reduce noisy CUDA plugin logs (optional)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:01:22.898987Z","iopub.execute_input":"2026-02-10T10:01:22.899304Z","iopub.status.idle":"2026-02-10T10:01:45.768866Z","shell.execute_reply.started":"2026-02-10T10:01:22.89928Z","shell.execute_reply":"2026-02-10T10:01:45.768059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 1) Imports + Hugging Face Login\n# =========================\nimport os, json, time, re\nfrom typing import Optional, Dict, Any, List\nfrom collections import defaultdict\n\nimport requests\nfrom rapidfuzz import fuzz\nimport numpy as np\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport bitsandbytes as bnb\n\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nprint(\"numpy:\", np.__version__)\nprint(\"torch cuda:\", torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\nprint(\"transformers:\", transformers.__version__)\nprint(\"bitsandbytes:\", bnb.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:01:45.770763Z","iopub.execute_input":"2026-02-10T10:01:45.771012Z","iopub.status.idle":"2026-02-10T10:02:20.738587Z","shell.execute_reply.started":"2026-02-10T10:01:45.770981Z","shell.execute_reply":"2026-02-10T10:02:20.737821Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load TxGemma (4-bit NF4 with fp16 fallback)\n\nWe load TxGemma in **4-bit NF4** to fit on Kaggle‚Äôs Tesla T4 GPU.\nIf 4-bit quantization fails, we fall back to fp16.\n","metadata":{}},{"cell_type":"code","source":"print(f\"BitsAndBytes version: {bnb.__version__}\")\nprint(f\"Is CUDA available for BNB? {torch.cuda.is_available()}\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\nprint(\"‚úÖ Environment Verified. You can now load TxGemma.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:02:20.739684Z","iopub.execute_input":"2026-02-10T10:02:20.7403Z","iopub.status.idle":"2026-02-10T10:02:20.745663Z","shell.execute_reply.started":"2026-02-10T10:02:20.74027Z","shell.execute_reply":"2026-02-10T10:02:20.744863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nMODEL_ID = \"google/txgemma-9b-chat\"\n\ndef load_txgemma_submit_safe(model_id: str, token: str | None = None):\n    assert torch.cuda.is_available(), \"GPU required\"\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id, token=token, use_fast=True)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        token=token,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n\n    model.eval()\n    model.config.use_cache = False\n    if getattr(model, \"generation_config\", None) is not None:\n        model.generation_config.use_cache = False\n\n    return tokenizer, model, \"4bit_auto\"\n\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\ntokenizer, model, mode = load_txgemma_submit_safe(MODEL_ID, HF_TOKEN)\nprint(\"Loaded:\", MODEL_ID, \"|\", mode, \"| device:\", model.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:02:20.747424Z","iopub.execute_input":"2026-02-10T10:02:20.747758Z","iopub.status.idle":"2026-02-10T10:04:15.272673Z","shell.execute_reply.started":"2026-02-10T10:02:20.747719Z","shell.execute_reply":"2026-02-10T10:04:15.271836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"Reply with exactly: OK\"}]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n)\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=60, do_sample=False)\n\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:15.273713Z","iopub.execute_input":"2026-02-10T10:04:15.273998Z","iopub.status.idle":"2026-02-10T10:04:18.535395Z","shell.execute_reply.started":"2026-02-10T10:04:15.273972Z","shell.execute_reply":"2026-02-10T10:04:18.534665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"‚úÖ **Model smoke test passed**: TxGemma loaded successfully and can generate text.\n\nNext, the notebook will:\n1. Retrieve evidence from PubMed and ChEMBL\n2. Build evidence ‚Äúsnippets‚Äù\n3. Generate a citation-grounded report\n4. Validate citation coverage (trust score)\n","metadata":{}},{"cell_type":"markdown","source":"## ChEMBL retrieval (molecule resolution + bioactivity-based target summary)\n\nThis section:\n1) Resolves a drug name to a **ChEMBL molecule ID**\n2) Pulls **bioactivity records** for that molecule\n3) Aggregates evidence per target (record counts, assay counts, example values)\n4) Saves a compact JSON ‚Äúmolecule evidence pack‚Äù\n","metadata":{}},{"cell_type":"code","source":"CHEMBL_BASE = \"https://www.ebi.ac.uk/chembl/api/data\"\nCACHE_DIR = \"/kaggle/working/data_cache\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n\n# Words that often indicate \"a form\" rather than the parent/base compound\nFORM_HINTS = [\n    \"hydrochloride\", \"hcl\", \"sodium\", \"potassium\", \"calcium\",\n    \"monohydrate\", \"hydrate\", \"tartrate\", \"phosphate\", \"sulfate\",\n    \"mesylate\", \"maleate\", \"acetate\", \"bromide\", \"chloride\"\n]\n\ndef _cache_path(key: str) -> str:\n    safe = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", key.strip().lower())\n    return os.path.join(CACHE_DIR, safe + \".json\")\n\ndef _load_cache(key: str) -> Optional[dict]:\n    path = _cache_path(key)\n    if os.path.exists(path):\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    return None\n\ndef _save_cache(key: str, obj: dict):\n    path = _cache_path(key)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(obj, f, ensure_ascii=False, indent=2)\n\ndef chembl_molecule_search(query: str, max_results: int = 25, use_cache: bool = True):\n    cache_key = f\"chembl_molecule_search__{query}__{max_results}__json\"\n    if use_cache:\n        cached = _load_cache(cache_key)\n        if cached is not None: return cached\n\n    url = f\"{CHEMBL_BASE}/molecule/search\"\n    params = {\"q\": query, \"limit\": max_results}\n    headers = {\"Accept\": \"application/json\"}\n    \n    # Try up to 3 times if the server is glitchy\n    for attempt in range(3):\n        try:\n            r = requests.get(url, params=params, headers=headers, timeout=30)\n            if r.status_code == 200:\n                data = r.json()\n                if use_cache: _save_cache(cache_key, data)\n                return data\n            elif r.status_code == 500:\n                print(f\"‚ö†Ô∏è ChEMBL Server Error (500). Attempt {attempt+1}/3. Retrying...\")\n                time.sleep(2) # Wait before retrying\n        except Exception as e:\n            print(f\"üì° Connection error: {e}\")\n            time.sleep(2)\n            \n    return {\"molecules\": []} # Return empty result instead of crashing\n\ndef _norm(s: str) -> str:\n    return re.sub(r\"\\s+\", \" \", (s or \"\").strip().lower())\n\ndef _looks_like_form(name: str) -> bool:\n    n = _norm(name)\n    return any(h in n for h in FORM_HINTS)\n\ndef _score_candidate(query: str, preferred_name: str, chembl_id: str) -> float:\n    \"\"\"\n    Scores a candidate for \"best match\".\n    - Higher if preferred_name matches query closely\n    - Penalize salt/hydrate forms unless query explicitly includes them\n    \"\"\"\n    q = _norm(query)\n    pn = _norm(preferred_name)\n\n    # Similarity score\n    sim = fuzz.token_set_ratio(q, pn)  # 0..100\n    score = sim\n\n    # If query does NOT mention form hints but preferred_name does, penalize.\n    q_mentions_form = _looks_like_form(q)\n    pn_mentions_form = _looks_like_form(pn)\n    if pn_mentions_form and not q_mentions_form:\n        score -= 12  # mild penalty\n\n    # If the preferred name is missing, heavily penalize\n    if not pn:\n        score -= 30\n\n    # Small bump if query is substring of preferred_name\n    if q and q in pn:\n        score += 4\n\n    return score\n\ndef resolve_drug_to_chembl(drug_name: str, max_results: int = 25) -> Dict[str, Any]:\n    \"\"\"\n    Returns best ChEMBL molecule match + alternatives.\n    \"\"\"\n    raw = chembl_molecule_search(drug_name, max_results=max_results, use_cache=True)\n\n    # ChEMBL returns results typically in raw[\"molecules\"] or raw[\"molecules\"]-like structures\n    # We'll be defensive.\n    candidates = raw.get(\"molecules\") or raw.get(\"molecule\") or raw.get(\"molecules\", [])\n    if not isinstance(candidates, list):\n        # sometimes it's under \"molecules\"->\"molecules\"\n        candidates = candidates.get(\"molecules\", []) if isinstance(candidates, dict) else []\n\n    parsed = []\n    for c in candidates:\n        chembl_id = c.get(\"molecule_chembl_id\") or c.get(\"chembl_id\") or \"\"\n        pref_name = c.get(\"pref_name\") or c.get(\"preferred_name\") or \"\"\n        if not chembl_id:\n            continue\n        score = _score_candidate(drug_name, pref_name, chembl_id)\n        parsed.append({\"chembl_id\": chembl_id, \"preferred_name\": pref_name, \"score\": score, \"raw\": c})\n\n    if not parsed:\n        return {\n            \"query\": drug_name,\n            \"best_chembl_id\": None,\n            \"preferred_name\": None,\n            \"match_reason\": \"no_results\",\n            \"alternatives\": [],\n        }\n\n    parsed.sort(key=lambda x: x[\"score\"], reverse=True)\n    best = parsed[0]\n\n    # Determine match_reason\n    q = _norm(drug_name)\n    pn = _norm(best[\"preferred_name\"])\n    if q == pn:\n        reason = \"exact\"\n    elif fuzz.token_set_ratio(q, pn) >= 90:\n        reason = \"high_confidence_fuzzy\"\n    else:\n        reason = \"fuzzy\"\n\n    alternatives = [p[\"chembl_id\"] for p in parsed[1:6]]  # top 5 alternatives\n\n    return {\n        \"query\": drug_name,\n        \"best_chembl_id\": best[\"chembl_id\"],\n        \"preferred_name\": best[\"preferred_name\"],\n        \"match_reason\": reason,\n        \"alternatives\": alternatives,\n        \"top_matches_debug\": [\n            {\"chembl_id\": p[\"chembl_id\"], \"preferred_name\": p[\"preferred_name\"], \"score\": round(p[\"score\"], 2)}\n            for p in parsed[:8]\n        ],\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:18.536519Z","iopub.execute_input":"2026-02-10T10:04:18.536843Z","iopub.status.idle":"2026-02-10T10:04:18.554687Z","shell.execute_reply.started":"2026-02-10T10:04:18.536818Z","shell.execute_reply":"2026-02-10T10:04:18.554107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name in [\"donepezil\", \"memantine\", \"rivastigmine\", \"galantamine\"]:\n    out = resolve_drug_to_chembl(name)\n    print(name, \"=>\", out[\"best_chembl_id\"], \"|\", out[\"preferred_name\"], \"|\", out[\"match_reason\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:18.555615Z","iopub.execute_input":"2026-02-10T10:04:18.555929Z","iopub.status.idle":"2026-02-10T10:04:21.08316Z","shell.execute_reply.started":"2026-02-10T10:04:18.55588Z","shell.execute_reply":"2026-02-10T10:04:21.082361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resolve_drug_to_chembl(\"donepezil\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:21.084309Z","iopub.execute_input":"2026-02-10T10:04:21.084841Z","iopub.status.idle":"2026-02-10T10:04:21.090362Z","shell.execute_reply.started":"2026-02-10T10:04:21.084814Z","shell.execute_reply":"2026-02-10T10:04:21.089848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"HEADERS_JSON = {\"Accept\": \"application/json\"}\n\ndef chembl_get(endpoint: str, params=None, cache_key=None, use_cache=True, retries=3):\n    if params is None: params = {}\n    \n    # 1. Standardize Cache Key\n    if cache_key is None:\n        key = endpoint + \"__\" + \"__\".join([f\"{k}={v}\" for k, v in sorted(params.items())])\n        cache_key = \"chembl_get__\" + re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", key)\n\n    if use_cache:\n        cached = _load_cache(cache_key)\n        if cached is not None: return cached\n\n    url = CHEMBL_BASE + endpoint\n    \n    # 2. Implement Retry Logic\n    for attempt in range(retries):\n        try:\n            r = requests.get(url, params=params, headers=HEADERS_JSON, timeout=60)\n            \n            # If we hit a 500, wait and try again\n            if r.status_code == 500:\n                print(f\"‚ö†Ô∏è ChEMBL 500 Error at {endpoint}. Retrying in {2**attempt}s...\")\n                time.sleep(2 ** attempt)\n                continue\n                \n            r.raise_for_status()\n            data = r.json()\n            \n            if use_cache:\n                _save_cache(cache_key, data)\n            time.sleep(0.2)\n            return data\n            \n        except requests.exceptions.HTTPError as e:\n            if attempt == retries - 1: # Last attempt failed\n                print(f\"‚ùå Permanent Failure for {endpoint}: {e}\")\n                # Return an empty dict so the pipeline doesn't crash\n                return {}\n            time.sleep(2)\n    return {}\n\ndef chembl_paginated(endpoint: str, params=None, limit_total=200, page_size=100, cache_prefix=\"\"):\n    \"\"\"\n    Fetches multiple pages from ChEMBL endpoints that return:\n      {\"page_meta\": {...}, \"<collection_name>\": [...]}\n    We'll stop when we collect limit_total items or no next page.\n    \"\"\"\n    if params is None:\n        params = {}\n    params = dict(params)\n    params[\"limit\"] = page_size\n    params[\"offset\"] = 0\n\n    all_items = []\n    while True:\n        cache_key = f\"{cache_prefix}{endpoint}__{params['offset']}__{page_size}__\" + \"__\".join([f\"{k}={v}\" for k, v in sorted(params.items())])\n        data = chembl_get(endpoint, params=params, cache_key=cache_key, use_cache=True)\n\n        # find the list payload (usually the only list value in the dict besides page_meta)\n        list_key = None\n        for k, v in data.items():\n            if isinstance(v, list):\n                list_key = k\n                break\n        if list_key is None:\n            break\n\n        batch = data.get(list_key, [])\n        all_items.extend(batch)\n\n        if len(all_items) >= limit_total:\n            break\n\n        page_meta = data.get(\"page_meta\", {})\n        next_url = page_meta.get(\"next\")\n        if not next_url or len(batch) == 0:\n            break\n\n        params[\"offset\"] += page_size\n\n    return all_items\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:21.091384Z","iopub.execute_input":"2026-02-10T10:04:21.092048Z","iopub.status.idle":"2026-02-10T10:04:21.108314Z","shell.execute_reply.started":"2026-02-10T10:04:21.092015Z","shell.execute_reply":"2026-02-10T10:04:21.107739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_molecule_evidence_pack(molecule_chembl_id: str, max_activities: int = 400):\n    \"\"\"\n    Creates a compact summary of:\n    - molecule details (name, SMILES if available)\n    - top targets based on bioactivity records\n    \"\"\"\n\n    # 1) Basic molecule info\n    mol = chembl_get(f\"/molecule/{molecule_chembl_id}\")\n    preferred_name = mol.get(\"pref_name\")\n    smiles = (mol.get(\"molecule_structures\") or {}).get(\"canonical_smiles\")\n    inchi = (mol.get(\"molecule_structures\") or {}).get(\"standard_inchi\")\n    molecule_type = mol.get(\"molecule_type\")\n\n    # 2) Pull bioactivities (this is the key factual base)\n    # We'll fetch up to max_activities to find targets + measurements.\n    activities = chembl_paginated(\n        \"/activity\",\n        params={\"molecule_chembl_id\": molecule_chembl_id},\n        limit_total=max_activities,\n        page_size=200,\n        cache_prefix=f\"act_{molecule_chembl_id}_\"\n    )\n\n    # 3) Aggregate activities per target\n    # We‚Äôll focus on records that have target_chembl_id and some measurement value.\n    per_target = defaultdict(lambda: {\n        \"target_chembl_id\": None,\n        \"target_pref_name\": None,\n        \"n_records\": 0,\n        \"types\": defaultdict(int),\n        \"values\": [],     # store a few representative numeric values\n        \"units\": set(),\n        \"standard_relation\": defaultdict(int),\n        \"standard_value_examples\": [],\n        \"assay_chembl_ids\": set(),\n        \"references\": set(),  # document_chembl_id\n    })\n\n    def safe_float(x):\n        try:\n            return float(x)\n        except:\n            return None\n\n    for a in activities:\n        tgt = a.get(\"target_chembl_id\")\n        if not tgt:\n            continue\n\n        std_type = a.get(\"standard_type\") or \"\"\n        std_value = safe_float(a.get(\"standard_value\"))\n        std_units = a.get(\"standard_units\") or \"\"\n        std_rel = a.get(\"standard_relation\") or \"\"\n        tgt_name = a.get(\"target_pref_name\") or \"\"\n\n        bucket = per_target[tgt]\n        bucket[\"target_chembl_id\"] = tgt\n        if tgt_name:\n            bucket[\"target_pref_name\"] = tgt_name\n        bucket[\"n_records\"] += 1\n        if std_type:\n            bucket[\"types\"][std_type] += 1\n        if std_units:\n            bucket[\"units\"].add(std_units)\n        if std_rel:\n            bucket[\"standard_relation\"][std_rel] += 1\n\n        if std_value is not None and len(bucket[\"values\"]) < 25:\n            bucket[\"values\"].append(std_value)\n            if len(bucket[\"standard_value_examples\"]) < 10:\n                bucket[\"standard_value_examples\"].append({\n                    \"type\": std_type,\n                    \"value\": std_value,\n                    \"units\": std_units,\n                    \"relation\": std_rel\n                })\n\n        assay_id = a.get(\"assay_chembl_id\")\n        if assay_id:\n            bucket[\"assay_chembl_ids\"].add(assay_id)\n\n        doc_id = a.get(\"document_chembl_id\")\n        if doc_id:\n            bucket[\"references\"].add(doc_id)\n\n    # 4) Score/sort targets: we‚Äôll use number of records as a simple importance proxy\n    targets_summary = []\n    for tgt, info in per_target.items():\n        targets_summary.append({\n            \"target_chembl_id\": info[\"target_chembl_id\"],\n            \"target_pref_name\": info[\"target_pref_name\"],\n            \"n_records\": info[\"n_records\"],\n            \"activity_types\": dict(sorted(info[\"types\"].items(), key=lambda x: x[1], reverse=True)),\n            \"units\": sorted(list(info[\"units\"]))[:5],\n            \"relation_counts\": dict(info[\"standard_relation\"]),\n            \"value_examples\": info[\"standard_value_examples\"],\n            \"n_assays\": len(info[\"assay_chembl_ids\"]),\n            \"n_references\": len(info[\"references\"]),\n        })\n\n    targets_summary.sort(key=lambda x: x[\"n_records\"], reverse=True)\n\n    pack = {\n        \"molecule_chembl_id\": molecule_chembl_id,\n        \"preferred_name\": preferred_name,\n        \"molecule_type\": molecule_type,\n        \"canonical_smiles\": smiles,\n        \"standard_inchi\": inchi,\n        \"n_activity_records_fetched\": len(activities),\n        \"top_targets\": targets_summary[:15],  # keep top 15 for now\n        \"generated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n\n    return pack\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:21.1108Z","iopub.execute_input":"2026-02-10T10:04:21.111041Z","iopub.status.idle":"2026-02-10T10:04:21.130423Z","shell.execute_reply.started":"2026-02-10T10:04:21.11102Z","shell.execute_reply":"2026-02-10T10:04:21.129876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pack_donepezil = build_molecule_evidence_pack(\"CHEMBL502\", max_activities=400)\npack_donepezil[\"preferred_name\"], pack_donepezil[\"n_activity_records_fetched\"], len(pack_donepezil[\"top_targets\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:21.131264Z","iopub.execute_input":"2026-02-10T10:04:21.131626Z","iopub.status.idle":"2026-02-10T10:04:24.224442Z","shell.execute_reply.started":"2026-02-10T10:04:21.131598Z","shell.execute_reply":"2026-02-10T10:04:24.223834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for t in pack_donepezil[\"top_targets\"][:8]:\n    print(t[\"target_chembl_id\"], \"|\", t[\"target_pref_name\"], \"| records:\", t[\"n_records\"], \"| types:\", list(t[\"activity_types\"].keys())[:3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:24.225404Z","iopub.execute_input":"2026-02-10T10:04:24.225691Z","iopub.status.idle":"2026-02-10T10:04:24.230359Z","shell.execute_reply.started":"2026-02-10T10:04:24.225667Z","shell.execute_reply":"2026-02-10T10:04:24.229805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/evidence_packs\", exist_ok=True)\n\nwith open(\"/kaggle/working/evidence_packs/donepezil_CHEMBL502.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(pack_donepezil, f, ensure_ascii=False, indent=2)\n\nprint(\"saved:\", \"/kaggle/working/evidence_packs/donepezil_CHEMBL502.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:24.231393Z","iopub.execute_input":"2026-02-10T10:04:24.231689Z","iopub.status.idle":"2026-02-10T10:04:24.250718Z","shell.execute_reply.started":"2026-02-10T10:04:24.231657Z","shell.execute_reply":"2026-02-10T10:04:24.250004Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PubMed retrieval (NCBI E-Utilities)\n\nThis section retrieves:\n- PubMed IDs via `esearch`\n- Article metadata + abstracts via `efetch`\n- A parsed list of papers used for evidence snippets ([S1], [S2], ...)\n","metadata":{}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\nNCBI_EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\nNCBI_TOOL = \"kaggle-txgemma-evidence-notebook\"\nNCBI_EMAIL = None  # optional (recommended by NCBI, but you can leave None)\n\ndef _ncbi_get(endpoint: str, params: dict, timeout=60, retries=4):\n    \"\"\"\n    Wrapper for NCBI E-utilities with polite rate limiting + retries.\n    Retries on transient failures / throttling.\n    \"\"\"\n    params = dict(params)\n    params[\"tool\"] = NCBI_TOOL\n    if NCBI_EMAIL:\n        params[\"email\"] = NCBI_EMAIL\n\n    url = f\"{NCBI_EUTILS}/{endpoint}\"\n\n    for attempt in range(retries):\n        try:\n            r = requests.get(url, params=params, timeout=timeout)\n\n            # Throttling / transient errors\n            if r.status_code in (429, 500, 502, 503):\n                wait = 2 ** attempt\n                print(f\"‚ö†Ô∏è NCBI {r.status_code} at {endpoint}. Retrying in {wait}s...\")\n                time.sleep(wait)\n                continue\n\n            r.raise_for_status()\n\n            # Polite rate limit (~3 req/sec)\n            time.sleep(0.34)\n            return r\n\n        except Exception as e:\n            if attempt == retries - 1:\n                raise\n            time.sleep(2 ** attempt)\n\n    raise RuntimeError(f\"NCBI request failed after {retries} retries: {endpoint}\")\n\ndef pubmed_esearch(term: str, retmax: int = 20, sort: str = \"relevance\"):\n    \"\"\"\n    Returns a list of PubMed IDs (PMIDs).\n    sort can be 'relevance' or 'date'.\n    \"\"\"\n    params = {\n        \"db\": \"pubmed\",\n        \"term\": term,\n        \"retmax\": retmax,\n        \"retmode\": \"json\",\n        \"sort\": sort,\n    }\n    r = _ncbi_get(\"esearch.fcgi\", params=params)\n    return r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n\ndef pubmed_efetch(pmids, rettype=\"abstract\"):\n    \"\"\"\n    Fetch details for a list of PMIDs.\n    Returns raw XML text.\n    \"\"\"\n    if not pmids:\n        return \"\"\n\n    params = {\n        \"db\": \"pubmed\",\n        \"id\": \",\".join(pmids),\n        \"retmode\": \"xml\",\n        \"rettype\": rettype,\n    }\n    r = _ncbi_get(\"efetch.fcgi\", params=params)\n    return r.text\n\ndef _findtext(el, path, default=\"\"):\n    if el is None:\n        return default\n    v = el.findtext(path)\n    return v.strip() if isinstance(v, str) else default\n\ndef parse_pubmed_xml(xml_text: str):\n    \"\"\"\n    Parses PubMed XML into a list of dicts:\n    pmid, title, abstract, journal, year, authors (+ optional extras)\n    \"\"\"\n    if not xml_text.strip():\n        return []\n\n    root = ET.fromstring(xml_text)\n    articles = []\n\n    for pubmed_article in root.findall(\".//PubmedArticle\"):\n        medline = pubmed_article.find(\"MedlineCitation\")\n        if medline is None:\n            continue\n        article = medline.find(\"Article\")\n        if article is None:\n            continue\n\n        pmid = _findtext(medline, \"PMID\", default=None)\n        title = _findtext(article, \"ArticleTitle\", default=\"\")\n\n        # Abstract (can be multi-part)\n        abstract_texts = []\n        abstract = article.find(\"Abstract\")\n        if abstract is not None:\n            for at in abstract.findall(\"AbstractText\"):\n                label = at.attrib.get(\"Label\")\n                txt = \"\".join(at.itertext()).strip()\n                if not txt:\n                    continue\n                abstract_texts.append(f\"{label}: {txt}\" if label else txt)\n\n        abstract_joined = \"\\n\".join(abstract_texts)\n\n        journal = _findtext(article, \"Journal/Title\", default=\"\")\n        year = (\n            _findtext(article, \"Journal/JournalIssue/PubDate/Year\", default=\"\")\n            or _findtext(article, \"Journal/JournalIssue/PubDate/MedlineDate\", default=\"\")\n        )\n\n        # Authors\n        authors = []\n        author_list = article.find(\"AuthorList\")\n        if author_list is not None:\n            for a in author_list.findall(\"Author\"):\n                collective = _findtext(a, \"CollectiveName\", default=\"\")\n                if collective:\n                    authors.append(collective)\n                    continue\n                fore = _findtext(a, \"ForeName\", default=\"\")\n                last = _findtext(a, \"LastName\", default=\"\")\n                full = (fore + \" \" + last).strip()\n                if full:\n                    authors.append(full)\n\n        # Optional: DOI (if present)\n        doi = \"\"\n        for aid in article.findall(\".//ArticleId\"):\n            if aid.attrib.get(\"IdType\") == \"doi\":\n                doi = (aid.text or \"\").strip()\n                break\n\n        # Optional: publication types (Review, Clinical Trial, etc.)\n        pub_types = [(_findtext(pt, \".\", default=\"\")) for pt in article.findall(\"PublicationTypeList/PublicationType\")]\n        pub_types = [p for p in pub_types if p]\n\n        articles.append({\n            \"pmid\": pmid,\n            \"title\": title,\n            \"abstract\": abstract_joined,\n            \"journal\": journal,\n            \"year\": year,\n            \"authors\": authors[:10],\n            \"doi\": doi,\n            \"pub_types\": pub_types[:8],\n            \"has_abstract\": bool(abstract_joined.strip()),\n        })\n\n    return articles\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:24.251881Z","iopub.execute_input":"2026-02-10T10:04:24.252334Z","iopub.status.idle":"2026-02-10T10:04:24.268695Z","shell.execute_reply.started":"2026-02-10T10:04:24.25231Z","shell.execute_reply":"2026-02-10T10:04:24.268117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_text_evidence_pack(\n    disease: str,\n    drug_name: str,\n    chembl_id: str = None,\n    n_papers: int = 20,\n    sort: str = \"relevance\", \n):\n    query = f'(\"{disease}\"[Title/Abstract]) AND (\"{drug_name}\"[Title/Abstract])'\n\n    CACHE_VERSION = \"v3\"  # bump since we added sort\n    cache_key = f\"pubmed_pack__{CACHE_VERSION}__{disease}__{drug_name}__{n_papers}__{sort}\"\n\n    cached = _load_cache(cache_key)\n    if cached is not None:\n        return cached\n\n    pmids = pubmed_esearch(query, retmax=n_papers, sort=sort)\n    if not pmids:\n        pack = {\n            \"disease\": disease,\n            \"drug_name\": drug_name,\n            \"chembl_id\": chembl_id,\n            \"query\": query,\n            \"sort\": sort,\n            \"pmids\": [],\n            \"papers\": [],\n            \"generated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n        _save_cache(cache_key, pack)\n        return pack\n\n    xml_text = pubmed_efetch(pmids)\n    papers = parse_pubmed_xml(xml_text)\n\n    # keep only papers with real abstracts\n    papers = [p for p in papers if (p.get(\"abstract\") or \"\").strip()]\n\n    pack = {\n        \"disease\": disease,\n        \"drug_name\": drug_name,\n        \"chembl_id\": chembl_id,\n        \"query\": query,\n        \"sort\": sort,\n        \"pmids\": pmids,\n        \"papers\": papers,\n        \"generated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n\n    _save_cache(cache_key, pack)\n    return pack\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:24.269606Z","iopub.execute_input":"2026-02-10T10:04:24.269969Z","iopub.status.idle":"2026-02-10T10:04:24.2873Z","shell.execute_reply.started":"2026-02-10T10:04:24.269939Z","shell.execute_reply":"2026-02-10T10:04:24.286765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"disease = \"Alzheimer\"\ndrug = \"donepezil\"\nchembl = \"CHEMBL502\"\n\ntext_pack = build_text_evidence_pack(\"Alzheimer disease\", \"donepezil\", chembl_id=\"CHEMBL502\", n_papers=25)\nlen(text_pack[\"papers\"]), text_pack[\"query\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:24.288117Z","iopub.execute_input":"2026-02-10T10:04:24.288435Z","iopub.status.idle":"2026-02-10T10:04:25.681633Z","shell.execute_reply.started":"2026-02-10T10:04:24.288414Z","shell.execute_reply":"2026-02-10T10:04:25.680942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for p in text_pack[\"papers\"][:5]:\n    print(\"PMID:\", p[\"pmid\"])\n    print(\"Year:\", p[\"year\"], \"| Journal:\", p[\"journal\"])\n    print(\"Title:\", p[\"title\"][:120])\n    print(\"Abstract snippet:\", (p[\"abstract\"][:200] + \"...\").replace(\"\\n\",\" \"))\n    print(\"-\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:25.682458Z","iopub.execute_input":"2026-02-10T10:04:25.682668Z","iopub.status.idle":"2026-02-10T10:04:25.687949Z","shell.execute_reply.started":"2026-02-10T10:04:25.682648Z","shell.execute_reply":"2026-02-10T10:04:25.687236Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json\nos.makedirs(\"/kaggle/working/text_packs\", exist_ok=True)\n\nout_path = \"/kaggle/working/text_packs/alzheimer_donepezil_textpack.json\"\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(text_pack, f, ensure_ascii=False, indent=2)\n\nprint(\"saved:\", out_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:25.688975Z","iopub.execute_input":"2026-02-10T10:04:25.689224Z","iopub.status.idle":"2026-02-10T10:04:25.706548Z","shell.execute_reply.started":"2026-02-10T10:04:25.689203Z","shell.execute_reply":"2026-02-10T10:04:25.70587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_snippets_from_text_pack(text_pack, max_snippets=12, abstract_char_limit=900):\n    papers = text_pack[\"papers\"][:max_snippets]\n    snippets = []\n    for i, p in enumerate(papers, start=1):\n        sid = f\"S{i}\"\n        title = (p.get(\"title\") or \"\").strip()\n        year = (p.get(\"year\") or \"\").strip()\n        journal = (p.get(\"journal\") or \"\").strip()\n        pmid = (p.get(\"pmid\") or \"\").strip()\n\n        abstract = (p.get(\"abstract\") or \"\").strip()\n        abstract = abstract.replace(\"\\n\", \" \")\n        if len(abstract) > abstract_char_limit:\n            abstract = abstract[:abstract_char_limit].rsplit(\" \", 1)[0] + \"...\"\n\n        snippet_text = (\n            f\"[{sid}] Title: {title}\\n\"\n            f\"Year: {year} | Journal: {journal} | PMID: {pmid}\\n\"\n            f\"Abstract: {abstract}\"\n        )\n        snippets.append({\"sid\": sid, \"pmid\": pmid, \"title\": title, \"year\": year, \"journal\": journal, \"text\": snippet_text})\n    return snippets\n\nsnippets = make_snippets_from_text_pack(text_pack, max_snippets=12)\nprint(\"snippets:\", len(snippets))\nprint(snippets[0][\"text\"][:600])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:25.707695Z","iopub.execute_input":"2026-02-10T10:04:25.708064Z","iopub.status.idle":"2026-02-10T10:04:25.728898Z","shell.execute_reply.started":"2026-02-10T10:04:25.70803Z","shell.execute_reply":"2026-02-10T10:04:25.72813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CITATION_RULES = \"\"\"\nYou are a neuroscience research evidence assistant.\n\nYou MUST follow these rules:\n1) Use ONLY the provided evidence snippets [S1], [S2], ... as sources.\n2) Every factual claim MUST include at least one citation like [S3].\n3) If evidence is missing or weak, explicitly say \"Insufficient evidence in the provided snippets\" and do NOT guess.\n4) Do NOT provide medical advice. Do NOT claim a treatment works. This is research support only.\n5) Distinguish clearly between:\n   - what is directly supported by snippets\n   - what is a hypothesis (label as \"Hypothesis\")\n6) Include an \"Uncertainty & Limitations\" section that mentions:\n   - evidence quality may vary (reviews vs experiments)\n   - abstracts are incomplete summaries\n\"\"\"\nprint(CITATION_RULES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:25.729999Z","iopub.execute_input":"2026-02-10T10:04:25.730618Z","iopub.status.idle":"2026-02-10T10:04:25.745157Z","shell.execute_reply.started":"2026-02-10T10:04:25.730593Z","shell.execute_reply":"2026-02-10T10:04:25.744395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"REPORT_TEMPLATE = \"\"\"\nReturn a structured report with these exact sections:\n\n1) Question\n- Restate the user's disease+molecule query in 1 sentence.\n\n2) Evidence Summary (with citations)\n- 4‚Äì8 bullet points summarizing what the snippets say about the molecule and disease.\n- Each bullet MUST end with citations like [S2][S5].\n\n3) Biological Rationale (with citations)\n- Explain plausible biological mechanisms mentioned in the snippets.\n- If you infer beyond the text, label it as Hypothesis and still cite supporting snippets.\n\n4) Contradictions / Gaps (with citations if applicable)\n- Note disagreements, missing info, or why evidence may not be strong.\n\n5) Uncertainty & Limitations\n- Include the required limitations.\n\n6) Safety Note\n- One short paragraph: not medical advice, not a validated therapeutic recommendation.\n\"\"\"\nprint(REPORT_TEMPLATE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:25.746227Z","iopub.execute_input":"2026-02-10T10:04:25.746605Z","iopub.status.idle":"2026-02-10T10:04:25.765477Z","shell.execute_reply.started":"2026-02-10T10:04:25.746574Z","shell.execute_reply":"2026-02-10T10:04:25.764917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompt Construction (Evidence ‚Üí Constrained Report)\n\nThis section builds the **single prompt** we send to TxGemma.\n\n### What goes into the prompt\n- **Citation rules**: the model must cite snippets like `[S1]`, `[S2]` for every factual claim\n- **Report template**: forces a stable structure (Question ‚Üí Evidence ‚Üí Mechanism ‚Üí Gaps ‚Üí Limitations)\n- **Evidence snippets**: compact PubMed abstracts we retrieved\n- **Optional molecular profile**: top ChEMBL target hints to ground mechanism discussion\n\n### Output\nA long string prompt that is later fed to `tokenizer.apply_chat_template()`.\n","metadata":{}},{"cell_type":"code","source":"def build_prompt(disease, drug, snippets, mol_pack=None):\n    \"\"\"\n    Builds a single prompt string that includes:\n    - citation rules (anti-hallucination constraints)\n    - report structure template\n    - evidence snippets (PubMed abstracts)\n    - optional molecular profile (ChEMBL context)\n    \"\"\"\n    evidence_block = \"\\n\\n\".join([s[\"text\"] for s in snippets])\n\n    # Optional: add ChEMBL-based context (helps biological rationale grounding)\n    mol_profile = \"\"\n    if mol_pack:\n        mol_profile = (\n            \"MOLECULAR PROFILE:\\n\"\n            f\"- ChEMBL ID: {mol_pack['molecule_chembl_id']}\\n\"\n            f\"- Top Targets: {', '.join([t['target_pref_name'] for t in mol_pack['top_targets'][:3]])}\\n\\n\"\n        )\n\n    # Final prompt assembled in a predictable order:\n    # rules -> template -> molecular profile -> snippets -> question\n    prompt = (\n        CITATION_RULES.strip() + \"\\n\\n\"\n        + REPORT_TEMPLATE.strip() + \"\\n\\n\"\n        + mol_profile\n        + \"EVIDENCE SNIPPETS:\\n\"\n        + evidence_block + \"\\n\\n\"\n        + f\"USER QUESTION: Write the research report for {drug} in {disease}.\\n\"\n        \"IMPORTANT FORMAT: In Section 2, write bullet points starting with '-' and END each bullet with citations like [S1][S2].\"\n        )\n    return prompt\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:25.766182Z","iopub.execute_input":"2026-02-10T10:04:25.76646Z","iopub.status.idle":"2026-02-10T10:04:25.780408Z","shell.execute_reply.started":"2026-02-10T10:04:25.76642Z","shell.execute_reply":"2026-02-10T10:04:25.779878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Report Generation (TxGemma Inference)\n\nHere we call TxGemma using the **chat template** and generate the report.\n\n### Notes\n- `do_sample=False` keeps output deterministic (useful for debugging)\n- `repetition_penalty` reduces loops and repeated text\n- Output is decoded from tokens **after** the prompt portion (so we return only the generated report)","metadata":{}},{"cell_type":"code","source":"\ndef generate_report(disease: str, drug: str, snippets, mol_pack=None, max_new_tokens: int = 900):\n    \"\"\"\n    Runs TxGemma inference:\n    - wraps prompt into a chat-style message\n    - uses apply_chat_template for Gemma-compatible formatting\n    - returns only the generated continuation (not the prompt)\n    \"\"\"\n    prompt = build_prompt(disease, drug, snippets, mol_pack=mol_pack)\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    # Tokenize with chat template to match model expectations\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=2048,   # or 1536 if still OOM\n    )\n\n    # Move tensors to the model device (GPU most of the time)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=384,\n            do_sample=False,\n            repetition_penalty=1.05,\n        )\n\n    # Decode only the generated tokens after the prompt tokens\n    return tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n        skip_special_tokens=True\n    )\n\nreport = generate_report(\"Alzheimer disease\", \"donepezil\", snippets)\nprint(report[:2000])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T10:04:25.781189Z","iopub.execute_input":"2026-02-10T10:04:25.781464Z","execution_failed":"2026-02-10T12:34:45.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exporting Reports to Markdown\n\nEach run of the pipeline produces a `result` dictionary.\nThis function saves it as a `.md` file in:\n\n`/kaggle/working/reports_md/`\n\nThis makes results portable:\n- easy to publish on GitHub\n- easy to attach to a Kaggle notebook output\n- easy to compare reports across drugs","metadata":{}},{"cell_type":"code","source":"def save_markdown_report(res, out_dir=\"/kaggle/working/reports_md\"):\n    \"\"\"\n    Saves one pipeline result into a clean Markdown file.\n    This is useful for sharing outputs outside Kaggle.\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    meta = res.get(\"metadata\", {})\n    drug = meta.get(\"drug\", \"UnknownDrug\").replace(\" \", \"_\")\n    disease = meta.get(\"disease\", \"UnknownDisease\").replace(\" \", \"_\")\n    fname = f\"{disease}_{drug}.md\"\n    path = os.path.join(out_dir, fname)\n\n    # \"trust_score\" is your citation-coverage grounding metric\n    score = res.get(\"citation_coverage_pct\", res.get(\"trust_score\", 0.0))\n\n    lines = []\n    lines.append(f\"# Research Report: {meta.get('drug')} in {meta.get('disease')}\")\n    lines.append(\"\")\n    lines.append(f\"- ChEMBL ID: {meta.get('chembl_id')}\")\n    lines.append(f\"- Citation coverage: {score}%\")\n    lines.append(\"\")\n    lines.append(\"---\")\n    lines.append(\"\")\n    lines.append(res.get(\"report\", \"*No report text generated.*\"))\n    lines.append(\"\")\n    lines.append(\"---\")\n    lines.append(\"## Sources\")\n\n    # Sources = PubMed snippet references\n    for s in res.get(\"sources\", []):\n        sid = s.get(\"sid\", \"S?\")\n        pmid = s.get(\"pmid\", \"\")\n        if pmid:\n            lines.append(f\"- {sid}: PMID {pmid} ‚Äî https://pubmed.ncbi.nlm.nih.gov/{pmid}/\")\n        else:\n            lines.append(f\"- {sid}\")\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(lines))\n\n    return path","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Citation Validation (Trust Score)\n\nThis validator checks whether the generated report **actually cites the provided snippets**.\n\n### What it measures\n- How many bullet-like lines exist\n- How many are missing snippet citations\n- Whether the report references invalid snippet IDs (e.g. `[S99]` when only 10 snippets exist)\n\n### Output\nA metrics dictionary. We convert coverage into a simple **Trust Score** (%).\nThis is not \"scientific truth\" ‚Äî it's a **grounding / provenance** indicator.\n","metadata":{}},{"cell_type":"markdown","source":"## Orchestrator (End-to-End Pipeline)\n\nThis is the ‚Äúglue‚Äù that runs the full system:\n\n1. Resolve drug name ‚Üí ChEMBL ID  \n2. (Optional) Pull molecular evidence pack from ChEMBL  \n3. Retrieve PubMed abstracts ‚Üí build evidence snippets  \n4. Generate report with TxGemma  \n5. Validate citations ‚Üí Trust Score  \n6. Return a structured result dict (report + metadata + sources)\n","metadata":{}},{"cell_type":"code","source":"def validate_citations(report_text: str, snippets: Optional[List[Dict[str, Any]]] = None):\n    \"\"\"\n    Citation validator for generated reports.\n\n    Goal: estimate \"grounding quality\" by checking whether bullet-like claims\n    include snippet citations such as [S1], [S2], etc.\n    \"\"\"\n    # Infer maximum snippet id available in this run (e.g. S1..S10)\n    if snippets:\n        max_sid = max(int(s[\"sid\"].replace(\"S\", \"\")) for s in snippets if \"sid\" in s)\n    else:\n        max_sid = 0\n\n    lines = report_text.splitlines()\n\n    # Accept multiple bullet styles: *, -, 1)\n    bullet_lines = [\n        l.strip()\n        for l in lines\n        if re.match(r\"^(\\*|-|\\d+\\))\\s+\", l.strip())\n    ]\n\n    missing = []\n    cited_nums = []\n\n    # For each bullet-like line, check if it contains any [S#] references\n    for b in bullet_lines:\n        matches = re.findall(r\"S(\\d+)\", b)\n        if not matches:\n            missing.append(b)\n        else:\n            cited_nums.extend(int(m) for m in matches)\n\n    # Any references outside 1..max_sid are invalid\n    bad_refs = sorted({n for n in cited_nums if max_sid and (n < 1 or n > max_sid)})\n\n    coverage_pct = round(\n        ((len(bullet_lines) - len(missing)) / len(bullet_lines)) * 100, 2\n    ) if bullet_lines else 0.0\n\n    return {\n        \"n_bullets\": len(bullet_lines),\n        \"n_missing_citations\": len(missing),\n        \"coverage_pct\": coverage_pct,\n        \"bad_reference_nums\": bad_refs,\n        \"missing_examples\": missing[:5],\n    }\n\n\ndef research_pipeline_orchestrator(disease_name, drug_name):\n    \"\"\"\n    End-to-end pipeline:\n    - drug -> ChEMBL resolve\n    - optional molecular pack\n    - PubMed retrieval -> snippets\n    - TxGemma report generation\n    - citation validation -> trust score\n    \"\"\"\n    print(f\"--- üîé Investigating {drug_name} for {disease_name} ---\")\n\n    # 1) Resolve drug -> ChEMBL\n    drug_info = resolve_drug_to_chembl(drug_name)\n    chembl_id = drug_info.get(\"best_chembl_id\")\n\n    # 1.5) Optional: build molecular evidence pack for mechanistic context\n    mol_pack = None\n    if chembl_id:\n        try:\n            mol_pack = build_molecule_evidence_pack(chembl_id, max_activities=400)\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Could not build molecule evidence pack for {chembl_id}: {repr(e)}\")\n            mol_pack = None\n\n    # 2) PubMed retrieval -> snippets\n    text_pack = build_text_evidence_pack(disease_name, drug_name, chembl_id=chembl_id, n_papers=25)\n    snippets = make_snippets_from_text_pack(text_pack, max_snippets=10)\n\n    if not snippets:\n        return {\n            \"error\": \"No snippets found\",\n            \"metadata\": {\"drug\": drug_name, \"disease\": disease_name, \"chembl_id\": chembl_id},\n        }\n\n    # 3) Generate report with TxGemma (pass mol_pack for extra grounding)\n    report_text = generate_report(disease_name, drug_name, snippets, mol_pack=mol_pack)\n\n    # 4) Validate citations -> trust score\n    v_results = validate_citations(report_text, snippets=snippets)\n    total_b = v_results.get(\"n_bullets\", 0)\n    missing_b = v_results.get(\"n_missing_citations\", 0)\n    trust_score = round(((total_b - missing_b) / total_b) * 100, 2) if total_b > 0 else 0\n\n    return {\n        \"metadata\": {\"disease\": disease_name, \"drug\": drug_name, \"chembl_id\": chembl_id},\n        \"molecule_pack\": mol_pack,\n        \"snippets\": snippets,\n        \"report\": report_text,\n        \"trust_score\": trust_score,\n        \"metrics\": v_results,\n        \"sources\": [{\"sid\": s[\"sid\"], \"pmid\": s[\"pmid\"], \"title\": s.get(\"title\", \"No Title\")} for s in snippets],\n    }","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Batch Run (Multiple Drug‚ÄìDisease Pairs)\n\nThis section runs the pipeline for multiple studies, saves:\n- per-study `.md` reports\n- a combined `.json` output containing all results\n\nThis is the main ‚Äúdemo‚Äù block that shows the notebook‚Äôs utility.\n","metadata":{}},{"cell_type":"code","source":"# Define your study list\nstudy_list = [\n    (\"Alzheimer Disease\", \"Donepezil\"),\n    (\"Parkinson Disease\", \"Levodopa\"),\n    (\"Multiple Sclerosis\", \"Fingolimod\")\n]\n\nfinal_outputs = []\n\nfor disease, drug in study_list:\n    result = research_pipeline_orchestrator(disease, drug)\n    final_outputs.append(result)\n\n    md_path = save_markdown_report(result)\n    print(\"Saved:\", md_path)\n    \n    # Save as we go to avoid losing data if the notebook times out\n    with open(\"/kaggle/working/final_research_results.json\", \"w\") as f:\n        json.dump(final_outputs, f, indent=4)\n\nprint(\"\\n‚úÖ All reports generated and saved to /kaggle/working/final_research_results.json\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\n# 1. Check if the list is empty first\nif final_outputs:\n    # 2. Iterate through all outputs instead of just the first one\n    for res in final_outputs:\n        meta = res.get('metadata', {})\n        drug = meta.get('drug', 'Unknown Drug')\n        disease = meta.get('disease', 'Unknown Disease')\n        \n        # Safely get the trust score from the new key 'trust_score'\n        # Default to 0.0 if not found\n        score = res.get('trust_score', 0.0)\n        \n        display(Markdown(f\"## Research Report: {drug} in {disease}\"))\n        display(Markdown(f\"**Trust Score:** `{score}%` (Fact-checked against snippets)\"))\n        display(Markdown(\"---\"))\n        \n        # Display the report or an error message\n        display(Markdown(res.get('report', \"*No report generated for this study.*\")))\n        \n        # 3. Source References safely\n        display(Markdown(\"### üìö Source References\"))\n        sources = res.get('sources', [])\n        if sources:\n            for src in sources:\n                sid = src.get('sid', 'S?')\n                pmid = src.get('pmid', '')\n                title = src.get('title', 'No Title Available')\n                display(Markdown(f\"- **{sid}**: {title} (PMID: [{pmid}](https://pubmed.ncbi.nlm.nih.gov/{pmid}/))\"))\n        else:\n            display(Markdown(\"*No specific citations were found in the context.*\"))\n            \n        display(Markdown(\"<br><br>\")) # Visual separator between drug reports\nelse:\n    print(\"‚ö†Ô∏è No data found in final_outputs. Ensure your orchestrator loop ran successfully.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_target_density(mol_pack):\n    \"\"\"Generates the ChEMBL bioactivity evidence chart.\"\"\"\n    if not mol_pack or not mol_pack.get('top_targets'):\n        print(\"‚ö†Ô∏è No molecular target data available for plotting.\")\n        return\n        \n    targets = [t['target_pref_name'] for t in mol_pack['top_targets'][:5]]\n    counts = [t['n_records'] for t in mol_pack['top_targets'][:5]]\n    \n    plt.figure(figsize=(10, 5))\n    plt.barh(targets, counts, color='teal')\n    plt.xlabel('Number of Bioactivity Records in ChEMBL')\n    plt.title(f\"Evidence Density for {mol_pack.get('preferred_name', 'Unknown')}\")\n    plt.gca().invert_yaxis()\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_impact_analysis(final_outputs):\n    \"\"\"Generates the numerical evidence for submission.\"\"\"\n    data = []\n    for res in final_outputs:\n        if 'metadata' in res:\n            data.append({\n                \"Drug\": res['metadata']['drug'],\n                \"Trust Score\": res.get('trust_score', 0),\n                \"Source Count\": len(res.get('sources', []))\n            })\n    \n    df = pd.DataFrame(data)\n    \n    # Plotting Trust vs Data Density\n    fig, ax1 = plt.subplots(figsize=(10, 5))\n    df.plot(x=\"Drug\", y=\"Trust Score\", kind=\"bar\", ax=ax1, color=\"#008080\", alpha=0.7)\n    ax1.set_ylabel(\"Trust Score (%)\", fontweight='bold')\n    ax1.set_ylim(0, 110)\n    plt.title(\"Numerical Evidence: Grounding Reliability across Therapeutic Classes\", fontsize=14)\n    plt.grid(axis='y', linestyle='--', alpha=0.6)\n    plt.show()\n\nplot_impact_analysis(final_outputs)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What makes this notebook useful?\n\nMany LLM medical summaries are not auditable.  \nThis notebook adds *provenance*:\n\n- Every claim should cite PubMed snippet IDs `[S1]...[S10]`\n- You can click each PMID to verify the abstract\n- A Trust Score estimates how consistently the model grounds claims in retrieved evidence\n\nThis makes it easier to:\n- create fast literature snapshots\n- compare evidence across drugs\n- generate structured reports you can audit\n","metadata":{}},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import display, clear_output, Markdown\n\n# --- üöÄ INTERACTIVE RESEARCH CONSOLE ---\n\n# 1. UI Elements\ndrug_input = widgets.Text(\n    value='Donepezil',\n    placeholder='Enter drug name...',\n    description='**Drug:**',\n    style={'description_width': 'initial'}\n)\n\ndisease_input = widgets.Text(\n    value='Alzheimer Disease',\n    placeholder='Enter disease...',\n    description='**Disease:**',\n    style={'description_width': 'initial'}\n)\n\nrun_button = widgets.Button(\n    description='Generate Research Report',\n    button_style='success', # Green button\n    tooltip='Click to run the full RAG pipeline',\n    icon='search'\n)\n\nui_output = widgets.Output()\n\n# 2. Function to link the UI to your existing code\ndef on_generate_clicked(b):\n    with ui_output:\n        clear_output()\n        drug = drug_input.value.strip()\n        disease = disease_input.value.strip()\n        \n        if not drug or not disease:\n            print(\"‚ö†Ô∏è Please enter both a drug and a disease.\")\n            return\n            \n        print(f\"üß¨ Initializing Evidence Synthesis for {drug} in {disease}...\")\n        \n        # Call your main pipeline function\n        result = research_pipeline_orchestrator(disease, drug)\n        \n        # Display the result using your existing display logic\n        if \"report\" in result:\n            display(Markdown(f\"## Report Results for {drug}\"))\n            display(Markdown(f\"**Trust Score:** `{result['trust_score']}%`\"))\n            display(Markdown(result['report']))\n            \n            # Optional: Show the plots immediately\n            plot_target_density(result['molecule_pack'])\n        else:\n            print(f\"‚ùå Error: {result.get('error', 'Unknown issue')}\")\n\n# 3. Connect and Display\nrun_button.on_click(on_generate_clicked)\n\ndisplay(Markdown(\"### üõ†Ô∏è MedGemma Evidence Explorer\"))\ndisplay(widgets.VBox([\n    widgets.HBox([drug_input, disease_input]), \n    run_button, \n    ui_output\n]))","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Agentic Workflow + Edge Inference\n\nThis section demonstrates two extensions:\n\nAgentic Workflow Extension (Self-Correction Loop)\nThe system generates a report, validates citation coverage, then automatically rewrites the report to repair missing citations while staying grounded in the same retrieved evidence.\n\nEdge Inference Extension (GGUF / llama.cpp)\nThe same evidence snippets and prompt are used, but inference is performed using a quantized GGUF model through llama.cpp to demonstrate local/edge feasibility.\n\nThese extensions do not change retrieval. They modify only the generation stage (agentic repair or edge backend), allowing a direct comparison with the baseline pipeline.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef extract_section(text: str, start_pat: str, end_pat: str):\n    \"\"\"\n    Returns (section_text, start_idx, end_idx). If not found, returns (\"\", -1, -1).\n    \"\"\"\n    m_start = re.search(start_pat, text, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n    if not m_start:\n        return \"\", -1, -1\n    start_idx = m_start.end()\n\n    m_end = re.search(end_pat, text[start_idx:], flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n    end_idx = start_idx + (m_end.start() if m_end else len(text) - start_idx)\n\n    return text[start_idx:end_idx], start_idx, end_idx\n\n\ndef split_bullets(section_text: str) -> List[str]:\n    \"\"\"\n    Extract lines that are bullets '- ...' from a section.\n    \"\"\"\n    bullets = []\n    for line in section_text.splitlines():\n        line = line.strip()\n        if line.startswith(\"- \"):\n            bullets.append(line)\n    return bullets\n\n\ndef has_snippet_citation(bullet: str) -> bool:\n    \"\"\"\n    True if bullet contains [S<number>]\n    \"\"\"\n    return bool(re.search(r\"\\[S\\d+\\]\", bullet))\n\n\ndef normalize_bullet_line(line: str) -> str:\n    \"\"\"\n    Ensure bullet begins with '- ' and strip whitespace.\n    \"\"\"\n    line = (line or \"\").strip()\n    if not line.startswith(\"- \"):\n        line = \"- \" + line.lstrip(\"-\").strip()\n    return line","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def repair_evidence_bullets(\n    disease: str,\n    drug: str,\n    snippets: List[Dict[str, Any]],\n    bullets_to_fix: List[str],\n    max_new_tokens: int = 350\n) -> List[str]:\n    \"\"\"\n    Repairs ONLY the provided bullets.\n    Returns EXACTLY the same number of bullets, in the same order.\n    \"\"\"\n    max_sid = max(int(s[\"sid\"].replace(\"S\", \"\")) for s in snippets)\n    evidence = \"\\n\\n\".join(s[\"text\"] for s in snippets)\n\n    bullets_block = \"\\n\".join(\n        f\"{i+1}) {b[2:].strip() if b.startswith('- ') else b.strip()}\"\n        for i, b in enumerate(bullets_to_fix)\n    )\n\n    prompt = f\"\"\"\nYou are fixing ONLY the Evidence Summary bullets for a grounded biomedical report.\n\nCONSTRAINTS:\n- Use ONLY citations [S1]..[S{max_sid}] taken from the provided snippets.\n- Do NOT introduce new topics. Stay strictly about: Drug={drug} and Disease={disease}.\n- You MUST output exactly {len(bullets_to_fix)} bullets, numbered 1)..{len(bullets_to_fix)}).\n- For each bullet:\n  - If supported by snippets, keep meaning and add citation(s) at the end like [S3] or [S2][S5].\n  - If not supported, replace with: \"Insufficient evidence in provided snippets.\" plus ONE citation to the closest snippet.\n- Output ONLY the numbered bullets, one per line. No extra text.\n\nEVIDENCE SNIPPETS:\n{evidence}\n\nBULLETS TO FIX:\n{bullets_block}\n\"\"\".strip()\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True, tokenize=True,\n        return_dict=True, return_tensors=\"pt\",\n    )\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            repetition_penalty=1.05,\n        )\n\n    repaired = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n\n    # Parse numbered lines like \"1) ...\"\n    repaired_lines = []\n    for line in repaired.splitlines():\n        line = line.strip()\n        m = re.match(r\"^\\s*\\d+\\)\\s+(.*)$\", line)\n        if m:\n            repaired_lines.append(normalize_bullet_line(m.group(1)))\n\n    # Enforce exact length: pad or truncate safely\n    if len(repaired_lines) < len(bullets_to_fix):\n        # fallback: keep original bullets for missing outputs (but normalized)\n        for i in range(len(repaired_lines), len(bullets_to_fix)):\n            repaired_lines.append(normalize_bullet_line(bullets_to_fix[i]))\n    elif len(repaired_lines) > len(bullets_to_fix):\n        repaired_lines = repaired_lines[:len(bullets_to_fix)]\n\n    return repaired_lines\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_hard_case(result, min_snippets=6, min_coverage=95.0):\n    \"\"\"\n    Decide if we need agentic repair.\n    Triggers when:\n    - retrieval is weak (few snippets)\n    - baseline citation coverage is below threshold\n    - baseline has bad snippet references\n    \"\"\"\n    snippets = result.get(\"snippets\") or []\n    metrics = result.get(\"metrics\") or {}\n\n    if len(snippets) < min_snippets:\n        return True\n\n    coverage = metrics.get(\"coverage_pct\", result.get(\"trust_score\", 0.0))\n    bad_refs = metrics.get(\"bad_reference_nums\", [])\n\n    if coverage < min_coverage:\n        return True\n    if bad_refs:\n        return True\n\n    return False","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def patch_report_evidence_summary(\n    report_text: str,\n    repaired_map: Dict[int, str]\n) -> str:\n    \"\"\"\n    Replace Evidence Summary bullets by their index (0-based) using repaired_map.\n    Only touches section 2.\n    \"\"\"\n    section_text, start_idx, end_idx = extract_section(\n        report_text,\n        start_pat=r\"(^|\\n)\\s*(?:#{1,6}\\s*)?(?:\\*\\*)?\\s*2\\)\\s*Evidence Summary.*?:?\\s*\",\n        end_pat=r\"(^|\\n)\\s*(?:#{1,6}\\s*)?(?:\\*\\*)?\\s*3\\)\\s*Biological Rationale\"\n    )\n\n    # If we can't locate the section, do nothing (avoid corrupting report)\n    if start_idx == -1:\n        return report_text\n\n    lines = section_text.splitlines()\n    bullet_positions = [i for i, l in enumerate(lines) if l.strip().startswith(\"- \")]\n    bullets = [lines[i].strip() for i in bullet_positions]\n\n    # Apply replacements\n    for b_idx, new_bullet in repaired_map.items():\n        if 0 <= b_idx < len(bullets):\n            bullets[b_idx] = normalize_bullet_line(new_bullet)\n\n    # Write bullets back into section lines\n    for j, pos in enumerate(bullet_positions):\n        lines[pos] = bullets[j]\n\n    new_section = \"\\n\".join(lines)\n    return report_text[:start_idx] + new_section + report_text[end_idx:]\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_report_from_prompt(prompt: str, max_new_tokens: int = 900) -> str:\n    \"\"\"\n    Single, reusable generation path for TxGemma (Transformers backend).\n    This mirrors your generate_report() logic but takes a raw prompt string.\n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\",\n    )\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            repetition_penalty=1.05,\n        )\n\n    return tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n        skip_special_tokens=True\n    )","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_evidence_summary_bullets(report_text: str) -> List[str]:\n    \"\"\"\n    Extract ONLY Evidence Summary bullets (Section 2).\n    Falls back to all '- ' bullets if section markers aren't found.\n    \"\"\"\n    sec2_text, _, _ = extract_section(\n        report_text,\n        start_pat=r\"(^|\\n)\\s*(?:#{1,6}\\s*)?(?:\\*\\*)?\\s*2\\)\\s*Evidence Summary.*?:?\\s*\",\n        end_pat=r\"(^|\\n)\\s*(?:#{1,6}\\s*)?(?:\\*\\*)?\\s*3\\)\\s*Biological Rationale\"\n    )\n    if sec2_text:\n        return split_bullets(sec2_text)\n    return [l.strip() for l in report_text.splitlines() if l.strip().startswith(\"- \")]\n\n\ndef validate_bullets_only(bullets: List[str], snippets: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Citation coverage on a list of bullets only.\n    \"\"\"\n    if not snippets:\n        return {\"n_bullets\": len(bullets), \"n_missing_citations\": len(bullets), \"coverage_pct\": 0.0,\n                \"bad_reference_nums\": [], \"missing_examples\": bullets[:5]}\n\n    max_sid = max(int(s[\"sid\"].replace(\"S\", \"\")) for s in snippets if \"sid\" in s)\n\n    missing = [b for b in bullets if not re.search(r\"\\[S\\d+\\]\", b)]\n    cited_nums = [int(n) for n in re.findall(r\"\\[S(\\d+)\\]\", \"\\n\".join(bullets))]\n    bad_refs = sorted({n for n in cited_nums if n < 1 or n > max_sid})\n\n    coverage_pct = round(((len(bullets) - len(missing)) / len(bullets)) * 100, 2) if bullets else 0.0\n\n    return {\n        \"n_bullets\": len(bullets),\n        \"n_missing_citations\": len(missing),\n        \"coverage_pct\": coverage_pct,\n        \"bad_reference_nums\": bad_refs,\n        \"missing_examples\": missing[:5],\n    }","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1) Agentic Workflow Extension: Self-Correction Loop\n\nWe treat the baseline pipeline as a ‚Äúfirst draft‚Äù.\nThen we run a self-correction loop that:\n\nvalidates citation coverage (bullet-level),\n\nidentifies bullets missing citations and invalid snippet references,\n\nrewrites the report without introducing new unsupported claims,\n\nstops once citation coverage meets a target threshold (e.g., 90%).","metadata":{}},{"cell_type":"code","source":"def agentic_research_pipeline(\n    disease: str,\n    drug: str,\n    max_retries: int = 3,\n    target_coverage: float = 90.0,\n):\n    \"\"\"\n    Baseline stays unchanged.\n    Agentic triggers if OVERALL citation coverage < target_coverage.\n    Then it repairs ONLY bullets missing citations (any section), without adding new facts.\n    \"\"\"\n\n    # 0) Baseline\n    result = research_pipeline_orchestrator(disease, drug)\n    snippets = result.get(\"snippets\")\n    if not snippets:\n        result[\"agentic_used\"] = False\n        result[\"agentic_attempts\"] = 0\n        return result\n\n    report = result.get(\"report\", \"\")\n\n    # 1) Baseline overall validation (this is what you were looking at: 83.33%, etc.)\n    v0_all = validate_citations(report, snippets=snippets)\n    result[\"metrics_all\"] = v0_all\n    result[\"trust_score\"] = v0_all.get(\"coverage_pct\", 0.0)\n\n    # Gate on OVERALL coverage now\n    if v0_all.get(\"coverage_pct\", 0.0) >= target_coverage and not v0_all.get(\"bad_reference_nums\"):\n        print(\"‚úÖ Baseline overall grounding already strong; agentic not needed.\")\n        result[\"agentic_used\"] = False\n        result[\"agentic_attempts\"] = 0\n        return result\n\n    # Helper: find bullet lines and their line indices\n    def find_bullets(report_text: str):\n        lines = report_text.splitlines()\n        bullet_positions = []\n        bullets = []\n        for i, line in enumerate(lines):\n            if line.strip().startswith(\"- \"):\n                bullet_positions.append(i)\n                bullets.append(line.strip())\n        return lines, bullet_positions, bullets\n\n    # Helper: does a bullet contain any [S#]?\n    def has_cite(b: str) -> bool:\n        return bool(re.search(r\"\\[S\\d+\\]\", b))\n\n    # Helper: repair a list of bullets via your existing repair prompt style\n    def repair_bullets_anywhere(bullets_to_fix: List[str]) -> List[str]:\n        max_sid = max(int(s[\"sid\"].replace(\"S\", \"\")) for s in snippets)\n        evidence = \"\\n\\n\".join(s[\"text\"] for s in snippets)\n\n        bullets_block = \"\\n\".join(\n            f\"{i+1}) {b[2:].strip() if b.startswith('- ') else b.strip()}\"\n            for i, b in enumerate(bullets_to_fix)\n        )\n\n        prompt = f\"\"\"\nYou are repairing ONLY bullet points that are missing citations in a grounded biomedical report.\n\nCONSTRAINTS:\n- Use ONLY citations [S1]..[S{max_sid}] from the provided snippets.\n- Do NOT introduce any new factual claims.\n- Keep each bullet's meaning if it is supported by snippets, and add best citation(s) at the end.\n- If not supported, replace the bullet with: \"Insufficient evidence in provided snippets.\" + ONE citation.\n- Output exactly {len(bullets_to_fix)} bullets, numbered 1)..{len(bullets_to_fix)}), one per line.\n- Output ONLY the numbered bullets (no extra text).\n\nEVIDENCE SNIPPETS:\n{evidence}\n\nBULLETS TO FIX:\n{bullets_block}\n\"\"\".strip()\n\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        inputs = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=True,\n            return_dict=True, return_tensors=\"pt\",\n        )\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=350,\n                do_sample=False,\n                repetition_penalty=1.05,\n            )\n\n        repaired = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n\n        repaired_lines = []\n        for line in repaired.splitlines():\n            line = line.strip()\n            m = re.match(r\"^\\s*\\d+\\)\\s+(.*)$\", line)\n            if m:\n                fixed = m.group(1).strip()\n                if not fixed.startswith(\"- \"):\n                    fixed = \"- \" + fixed.lstrip(\"-\").strip()\n                repaired_lines.append(fixed)\n\n        # enforce exact length\n        if len(repaired_lines) < len(bullets_to_fix):\n            for i in range(len(repaired_lines), len(bullets_to_fix)):\n                repaired_lines.append(bullets_to_fix[i])\n        repaired_lines = repaired_lines[:len(bullets_to_fix)]\n\n        return repaired_lines\n\n    # 2) Agentic loop\n    agentic_used = False\n    attempts = 0\n\n    for attempt in range(1, max_retries + 1):\n        lines, bullet_positions, bullets = find_bullets(report)\n        bad_idx = [i for i, b in enumerate(bullets) if not has_cite(b)]\n\n        if not bad_idx:\n            # nothing left to repair\n            break\n\n        bullets_to_fix = [bullets[i] for i in bad_idx]\n        print(f\"‚ö†Ô∏è Agentic attempt {attempt}: repairing {len(bullets_to_fix)} missing-citation bullets (whole report)...\")\n\n        repaired = repair_bullets_anywhere(bullets_to_fix)\n\n        # apply repairs back into report\n        for j, b_i in enumerate(bad_idx):\n            line_pos = bullet_positions[b_i]\n            lines[line_pos] = repaired[j]\n\n        report = \"\\n\".join(lines)\n\n        agentic_used = True\n        attempts += 1\n\n        v_all = validate_citations(report, snippets=snippets)\n        result[\"metrics_all\"] = v_all\n        result[\"trust_score\"] = v_all.get(\"coverage_pct\", 0.0)\n\n        if v_all.get(\"coverage_pct\", 0.0) >= target_coverage and not v_all.get(\"bad_reference_nums\"):\n            print(f\"‚úÖ Agentic PASS on attempt {attempt}: overall_coverage={v_all['coverage_pct']}%\")\n            break\n\n    # 3) finalize\n    result[\"report\"] = report\n    result[\"agentic_used\"] = agentic_used\n    result[\"agentic_attempts\"] = attempts\n    # Final metrics\n    result[\"metrics_all\"] = validate_citations(result[\"report\"], snippets=snippets)\n\n    sec2_bullets_final = get_evidence_summary_bullets(result[\"report\"])\n    result[\"metrics_sec2\"] = validate_bullets_only(sec2_bullets_final, snippets=snippets)\n\n    return result","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\ndef show_comparison(disease, drug, baseline_res, agentic_res, max_chars=1200):\n    \"\"\"\n    Robust display helper:\n    - Baseline overall coverage from baseline_res[\"metrics\"] if present, else baseline_res[\"trust_score\"]\n    - Agentic overall coverage from agentic_res[\"metrics_all\"] (preferred), else agentic_res[\"metrics\"], else trust_score\n    - Agentic Evidence Summary coverage from agentic_res[\"metrics_sec2\"] if present\n    \"\"\"\n\n    # -------------------------\n    # Baseline overall coverage\n    # -------------------------\n    b_metrics = baseline_res.get(\"metrics\")\n    if isinstance(b_metrics, dict):\n        b_all_cov = b_metrics.get(\"coverage_pct\", baseline_res.get(\"trust_score\", 0.0))\n    else:\n        b_all_cov = baseline_res.get(\"trust_score\", 0.0)\n\n    # -------------------------\n    # Agentic overall coverage\n    # Prefer metrics_all, fallback to metrics, fallback trust_score\n    # -------------------------\n    a_metrics_all = agentic_res.get(\"metrics_all\")\n    a_metrics = agentic_res.get(\"metrics\")\n\n    if isinstance(a_metrics_all, dict):\n        a_all_cov = a_metrics_all.get(\"coverage_pct\", agentic_res.get(\"trust_score\", 0.0))\n    elif isinstance(a_metrics, dict):\n        a_all_cov = a_metrics.get(\"coverage_pct\", agentic_res.get(\"trust_score\", 0.0))\n    else:\n        a_all_cov = agentic_res.get(\"trust_score\", 0.0)\n\n    # -------------------------\n    # Agentic Evidence Summary coverage (Section 2)\n    # -------------------------\n    a_sec2 = agentic_res.get(\"metrics_sec2\")\n    a_sec2_cov = a_sec2.get(\"coverage_pct\") if isinstance(a_sec2, dict) else None\n\n    # -------------------------\n    # Render\n    # -------------------------\n    display(Markdown(f\"## Baseline vs Agentic: **{drug}** in **{disease}**\"))\n    display(Markdown(f\"- **Baseline overall coverage:** `{b_all_cov}%`\"))\n    display(Markdown(f\"- **Agentic overall coverage:** `{a_all_cov}%`\"))\n    display(Markdown(f\"- **Agentic Evidence Summary coverage:** `{a_sec2_cov if a_sec2_cov is not None else 'N/A'}%`\"))\n    display(Markdown(f\"- **Agentic attempts:** `{agentic_res.get('agentic_attempts', 'N/A')}`\"))\n    display(Markdown(f\"- **Agentic used:** `{agentic_res.get('agentic_used', 'N/A')}`\"))\n    display(Markdown(\"---\"))\n\n    display(Markdown(\"### Baseline Report (first chunk)\"))\n    display(Markdown(\"```text\\n\" + (baseline_res.get(\"report\",\"\")[:max_chars]) + \"\\n```\"))\n\n    display(Markdown(\"### Agentic Report (first chunk)\"))\n    display(Markdown(\"```text\\n\" + (agentic_res.get(\"report\",\"\")[:max_chars]) + \"\\n```\"))","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Strong/easy demo (should often NOT trigger agentic)\neasy_cases = [\n    (\"Alzheimer Disease\", \"Donepezil\"),\n]\n\n# Harder cases (more likely to trigger agentic or force \"insufficient evidence\" behavior)\nhard_cases = [\n    (\"Alzheimer disease\", \"ibuprofen\"),\n    (\"ALS\", \"metformin\"),\n    (\"Parkinson disease\", \"isradipine\"),\n    (\"Glioblastoma\", \"propranolol\"),\n]\n\ncases_to_run = easy_cases + hard_cases\n\nfor disease_demo, drug_demo in cases_to_run:\n    baseline = research_pipeline_orchestrator(disease_demo, drug_demo)\n    agentic = agentic_research_pipeline(disease_demo, drug_demo, max_retries=3, target_coverage=90.0)\n\n    show_comparison(disease_demo, drug_demo, baseline, agentic)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2) Edge Inference Extension: GGUF / llama.cpp Backend\n\nThis section demonstrates inference using a quantized GGUF model (llama.cpp), which is suitable for running locally on a normal PC (CPU-only) or on modest GPUs.\n\nWe keep retrieval + prompting identical and swap only the generation backend.","metadata":{}},{"cell_type":"markdown","source":"## Edge Inference Mode (GGUF / llama.cpp)\n\nThis notebook supports running the same report-generation prompt using a locally quantized GGUF model via **llama.cpp**.\n\n**Kaggle limitation:** Edge inference will run only if a `.gguf` file is available under `/kaggle/input` (attached as a Kaggle Dataset).  \nIf no GGUF file is present, the notebook will **skip edge inference gracefully** and print **local run instructions**.\n\n**Local run workflow:**\n1. Obtain a GGUF model (e.g., Q4_K_M quantization).\n2. Install: `pip install llama-cpp-python`\n3. Set `GGUF_PATH` to your local `.gguf` file.\n4. Run `generate_report_edge()` with the same snippets used by the baseline pipeline.\n","metadata":{}},{"cell_type":"code","source":"!pip -q install llama-cpp-python\n\nimport os, time\n\ntry:\n    from llama_cpp import Llama\nexcept Exception as e:\n    Llama = None\n\ndef find_gguf_files(base=\"/kaggle/input\"):\n    ggufs = []\n    for root, _, files in os.walk(base):\n        for f in files:\n            if f.lower().endswith(\".gguf\"):\n                ggufs.append(os.path.join(root, f))\n    return ggufs\n\n# Global handle (None unless we successfully load)\nllm = None\nGGUF_PATH = None\n\ndef setup_edge_llm():\n    \"\"\"\n    Tries to find and load a GGUF model in Kaggle. If not found, prints local instructions.\n    \"\"\"\n    global llm, GGUF_PATH\n\n    if Llama is None:\n        print(\"‚ö†Ô∏è llama-cpp-python not available in this environment.\")\n        return\n\n    ggufs = find_gguf_files()\n    if not ggufs:\n        print(\"‚ö†Ô∏è No GGUF model available inside this Kaggle session.\")\n        print(\"This is OK for the writeup: you can still demonstrate 'edge readiness' by providing local run steps.\\n\")\n        print(\"Local run steps (example):\")\n        print(\"1) Put a GGUF model on your machine, e.g. ./models/medgemma-9b-q4_k_m.gguf\")\n        print(\"2) pip install llama-cpp-python\")\n        print(\"3) Set GGUF_PATH to your local file and run setup_edge_llm() again.\\n\")\n        return\n\n    GGUF_PATH = ggufs[0]\n    print(\"‚úÖ Found GGUF:\", GGUF_PATH)\n\n    llm = Llama(\n        model_path=GGUF_PATH,\n        n_ctx=2048,\n        n_gpu_layers=-1,   # set 0 for CPU-only\n        verbose=False\n    )\n    print(\"‚úÖ GGUF model loaded via llama.cpp\")\n\ndef run_on_edge(prompt, max_tokens=256):\n    \"\"\"\n    Safe wrapper: returns a dict even when edge isn't available.\n    \"\"\"\n    if llm is None:\n        raise RuntimeError(\n            \"Edge inference not available in Kaggle because no .gguf model was found in /kaggle/input.\\n\"\n            \"Attach a Kaggle Dataset containing a .gguf file OR run locally by setting GGUF_PATH to your local file.\"\n        )\n\n    t0 = time.time()\n    out = llm(\n        f\"User: {prompt}\\nAI:\",\n        max_tokens=max_tokens,\n        stop=[\"User:\"],\n        temperature=0.0\n    )\n    t1 = time.time()\n    return {\"text\": out[\"choices\"][0][\"text\"], \"seconds\": round(t1 - t0, 3)}\n\ndef generate_report_edge(disease: str, drug: str, snippets, mol_pack=None, max_tokens: int = 650) -> dict:\n    prompt = build_prompt(disease, drug, snippets, mol_pack=mol_pack)\n    return run_on_edge(prompt, max_tokens=max_tokens)\n\n# Call once\nsetup_edge_llm()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\ndef compare_transformers_vs_edge(disease, drug, max_tokens=650):\n    # -------------------------\n    # 1) Transformers baseline\n    # -------------------------\n    base = research_pipeline_orchestrator(disease, drug)\n    snips = base.get(\"snippets\", [])\n    mol_pack = base.get(\"molecule_pack\", None)\n\n    base_report = base.get(\"report\", \"\")\n    base_metrics = validate_citations(base_report, snippets=snips)\n\n    # -------------------------\n    # 2) Edge attempt (SAFE)\n    # Always define edge_out so the notebook never crashes\n    # -------------------------\n    edge_out = {\n        \"text\": \"\",\n        \"seconds\": None,\n        \"skipped\": True,\n        \"reason\": \"edge_not_available\"\n    }\n\n    try:\n        edge_out = generate_report_edge(\n            disease, drug, snips,\n            mol_pack=mol_pack,\n            max_tokens=max_tokens\n        )\n        # if your generate_report_edge/run_on_edge doesn't set these, force them:\n        edge_out.setdefault(\"skipped\", False)\n        edge_out.setdefault(\"reason\", None)\n    except RuntimeError as e:\n        # Kaggle session has no GGUF: this is expected\n        edge_out[\"skipped\"] = True\n        edge_out[\"reason\"] = str(e)\n\n    # -------------------------\n    # 3) Display comparison\n    # -------------------------\n    display(Markdown(\"## Transformers vs Edge (GGUF) Comparison\"))\n    display(Markdown(f\"- **Transformers citation coverage:** `{base_metrics.get('coverage_pct', 0.0)}%`\"))\n\n    if edge_out.get(\"skipped\", False):\n        display(Markdown(\"- **Edge (GGUF) citation coverage:** `SKIPPED (no GGUF in this Kaggle session)`\"))\n        display(Markdown(\"- **Edge runtime:** `N/A`\"))\n        display(Markdown(\"---\"))\n        display(Markdown(\"### Edge Report\"))\n        display(Markdown(\"> Skipped in this Kaggle session. This still demonstrates **edge readiness**.\"))\n        display(Markdown(\"#### Local run (PC) steps\"))\n        display(Markdown(\"\"\"\n1) Get a `.gguf` model file on your machine (example: `./models/medgemma-9b-q4_k_m.gguf`)  \n2) `pip install llama-cpp-python`  \n3) Set `GGUF_PATH` to that file and run the edge setup cell again  \n        \"\"\"))\n        # Optional: show the exact reason (helpful for judges / debugging)\n        display(Markdown(f\"**Skip reason:** `{edge_out.get('reason','')[:200]}...`\"))\n        return\n\n    # Otherwise, evaluate edge report normally\n    edge_report = edge_out.get(\"text\", \"\")\n    edge_metrics = validate_citations(edge_report, snippets=snips)\n\n    display(Markdown(f\"- **Edge (GGUF) citation coverage:** `{edge_metrics.get('coverage_pct', 0.0)}%`\"))\n    display(Markdown(f\"- **Edge runtime:** `{edge_out.get('seconds', 'N/A')}` seconds\"))\n    display(Markdown(\"---\"))\n    display(Markdown(\"### Edge Report (first ~1200 chars)\"))\n    display(Markdown(\"```text\\n\" + edge_report[:1200] + \"\\n```\"))\n\n\ncompare_transformers_vs_edge(\"Alzheimer Disease\", \"Donepezil\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß¨ Novel Task: LoRA Fine-Tuning for Drug‚ÄìDrug Interaction Prediction\n\nSo far we demonstrated:\n\n‚Ä¢ Grounded drug repurposing reports (agentic workflow)  \n‚Ä¢ Local/edge inference capability (GGUF)  \n\nIn this section we show a third capability:\n\nüëâ Rapidly adapting MedGemma to a **new biomedical task** using parameter-efficient fine-tuning (LoRA).\n\nWe transform the model into a **Drug‚ÄìDrug Interaction (DDI) predictor**.\n","metadata":{}},{"cell_type":"markdown","source":"## Why Drug‚ÄìDrug Interaction Prediction?\n\nDrug‚Äìdrug interactions are a major safety concern in clinical practice.  \nMany adverse events occur because multiple medications interact in harmful ways.\n\nIf a research assistant can:\n- understand literature\n- reason about pharmacology\n- and predict interactions\n\n‚Ä¶it becomes far more useful for real-world drug discovery and clinical decision support.\n\nThis section demonstrates how the same MedGemma backbone can be **rapidly specialized** into a pharmacological interaction model.\n","metadata":{}},{"cell_type":"markdown","source":"## Novel Task: LoRA Fine-Tuning for Drug‚ÄìDrug Interaction (DDI) Prediction\n\n### Goal\nTransform TxGemma from a general biomedical assistant into a **specialized DDI predictor** using **parameter-efficient fine-tuning (LoRA)**.\n\n### Why LoRA?\n- Updates only a small fraction of parameters (efficient + fast).\n- Enables rapid specialization on a new task without retraining the full 9B model.\n\n### Training Setup (Demo Scale)\n- **Model:** `google/txgemma-9b-chat` (4-bit quantized)\n- **Method:** LoRA adapters on attention + MLP projection layers  \n- **Dataset:** small instruction-style DDI examples (for demonstration)\n- **Objective:** causal language modeling (next-token prediction)","metadata":{}},{"cell_type":"markdown","source":"## Step 1 ‚Äî Creating a Mini DDI Training Dataset\n\nTo keep the notebook lightweight, we create a **toy dataset** of known drug‚Äìdrug interactions.\n\nIn a production system this step would use:\n- DDI Corpus\n- DrugBank interactions\n- FAERS pharmacovigilance data\n\nHere we only need a few examples to demonstrate the fine-tuning pipeline.\n","metadata":{}},{"cell_type":"code","source":"# --- MINI DDI DATASET (for demonstration) ---\nddi_examples = [\n    {\n        \"drug_a\": \"warfarin\",\n        \"drug_b\": \"aspirin\",\n        \"label\": \"increase bleeding risk\"\n    },\n    {\n        \"drug_a\": \"metformin\",\n        \"drug_b\": \"cimetidine\",\n        \"label\": \"increase metformin concentration\"\n    },\n    {\n        \"drug_a\": \"simvastatin\",\n        \"drug_b\": \"clarithromycin\",\n        \"label\": \"risk of rhabdomyolysis\"\n    },\n    {\n        \"drug_a\": \"lisinopril\",\n        \"drug_b\": \"spironolactone\",\n        \"label\": \"risk of hyperkalemia\"\n    },\n    {\n        \"drug_a\": \"ibuprofen\",\n        \"drug_b\": \"prednisone\",\n        \"label\": \"increased gastrointestinal toxicity\"\n    }\n]\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2 ‚Äî Converting Data to Instruction Format\n\nLarge language models learn best from **instruction-response pairs**.\n\nWe convert each DDI example into a prompt that teaches the model:\n\nInput ‚Üí two drugs  \nOutput ‚Üí predicted interaction\n","metadata":{}},{"cell_type":"code","source":"def format_ddi_example(ex):\n    return f\"\"\"\n### Instruction:\nPredict the pharmacological interaction between two drugs.\n\n### Input:\nDrug A: {ex['drug_a']}\nDrug B: {ex['drug_b']}\n\n### Response:\nCo-administration may {ex['label']}.\n\"\"\".strip()\n\nddi_texts = [format_ddi_example(e) for e in ddi_examples]\nddi_texts[:2]\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3 ‚Äî Tokenizing the Dataset\n\nWe convert the instruction examples into tokenized text suitable for model training.\n","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\nddi_dataset = Dataset.from_dict({\"text\": ddi_texts})\n\ndef tokenize_fn(example):\n    out = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=256,\n    )\n    out[\"labels\"] = out[\"input_ids\"].copy()\n    return out\n\ntokenized_ddi = ddi_dataset.map(tokenize_fn, remove_columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4 ‚Äî Preparing LoRA Fine-Tuning\n\nInstead of retraining the entire model, we use **LoRA (Low-Rank Adaptation)**.\n\nWhy LoRA?\n\n‚Ä¢ Trains only a tiny number of parameters  \n‚Ä¢ Very fast and cheap  \n‚Ä¢ Preserves the base model knowledge  \n‚Ä¢ Ideal for rapidly adding new biomedical tasks  \n\nThis is critical for real-world drug discovery workflows where new tasks appear frequently.\n","metadata":{}},{"cell_type":"code","source":"# --- FIX: PEFT / HF HUB compatibility for transformers==4.44.2 ---\n# Run this cell ONCE, then RESTART the kernel (Kaggle requirement).\n\n!pip -q uninstall -y peft\n!pip -q install -U \"huggingface_hub==0.24.6\" \"peft==0.12.0\"\n\nimport os\n#print(\"‚úÖ Installed pinned versions. Restarting kernel now...\")\n#os._exit(0)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import huggingface_hub, peft, transformers\nprint(\"huggingface_hub:\", huggingface_hub.__version__)\nprint(\"peft:\", peft.__version__)\nprint(\"transformers:\", transformers.__version__)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\nprint(model.print_trainable_parameters())\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5 ‚Äî Lightweight Fine-Tuning\n\nWe run a short training loop to demonstrate how MedGemma can be adapted to the DDI task.\n\nThis is a **proof-of-concept training run**, not a full training session.\n","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\nimport torch.nn as nn\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./ddi_lora\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,     # demo\n    learning_rate=2e-4,\n    logging_steps=1,\n    save_strategy=\"no\",\n    report_to=\"none\",\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ddi,\n    data_collator=data_collator,\n)\n\ntrainer.train()\nmodel.eval()\n\n# 1) Force *all* generation cache paths OFF\nmodel.config.use_cache = False\nif getattr(model, \"generation_config\", None) is not None:\n    model.generation_config.use_cache = False\n    # Gemma2 often defaults to sliding_window cache; disable it explicitly\n    if hasattr(model.generation_config, \"cache_implementation\"):\n        model.generation_config.cache_implementation = None\n\n# 2) prepare_model_for_kbit_training() made some layers fp32 (esp LayerNorm).\n#    For inference, cast LayerNorm back to fp16 so attention KV states stay fp16.\nfor m in model.modules():\n    if isinstance(m, nn.LayerNorm):\n        m.to(torch.float16)\n\n# Optional: also cast the lm_head if it exists and got promoted\nif hasattr(model, \"lm_head\") and getattr(model.lm_head, \"weight\", None) is not None:\n    model.lm_head.to(torch.float16)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6 ‚Äî Testing the New Capability\n\nAfter fine-tuning, we test the model on a new drug pair.\n\nThis demonstrates how the system can now perform a **completely new pharmacological task** that was not part of the original pipeline.\n","metadata":{}},{"cell_type":"code","source":"def predict_ddi(drug_a: str, drug_b: str, max_new_tokens: int = 80) -> str:\n    prompt = f\"\"\"### Instruction:\nPredict the pharmacological interaction between two drugs.\n\n### Input:\nDrug A: {drug_a}\nDrug B: {drug_b}\n\n### Response:\n\"\"\".strip()\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\",\n    )\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Hard override generation config (prevents sliding-cache getting created)\n    gen_cfg = model.generation_config\n    gen_cfg.use_cache = False\n    if hasattr(gen_cfg, \"cache_implementation\"):\n        gen_cfg.cache_implementation = None\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_cfg,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            repetition_penalty=1.05,\n        )\n\n    gen = out[0][inputs[\"input_ids\"].shape[-1]:]\n    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n\nprint(\"DDI prediction demo:\")\nprint(\"warfarin + aspirin ->\", predict_ddi(\"warfarin\", \"aspirin\"))\nprint(\"metformin + cimetidine ->\", predict_ddi(\"metformin\", \"cimetidine\"))\nprint(\"sertraline + tramadol ->\", predict_ddi(\"sertraline\", \"tramadol\"))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-10T12:34:45.774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üöÄ Impact: Expanding the System to New Biomedical Tasks\n\nWith LoRA fine-tuning, the same MedGemma backbone can support:\n\n‚Ä¢ Drug repurposing research  \n‚Ä¢ Literature grounding and citation verification  \n‚Ä¢ Edge/local inference on consumer hardware  \n‚Ä¢ Drug‚Äìdrug interaction prediction (new task)\n\nThis shows that the architecture is not a single-purpose pipeline but a **general biomedical AI platform** that can quickly adapt to new challenges.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}